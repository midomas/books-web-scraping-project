{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6913fb8b",
   "metadata": {},
   "source": [
    "# Multithreaded Web Scraping & Data Analysis\n",
    "\n",
    "Scraping books from **books.toscrape.com**, saving data to JSON, and analyzing it using Pandas.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Web scraping\n",
    "- Multithreading\n",
    "- JSON file handling\n",
    "- Data preprocessing & analysis with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6118b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a9e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BASE_URL = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "PAGES_TO_SCRAPE = 5\n",
    "OUTPUT_JSON_FILE = \"scraped_books.json\"\n",
    "\n",
    "scraped_data = []\n",
    "data_lock = threading.Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4b8f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- PART 1: SCRAPING ----------------\n",
    "def scrape_page(page_number):\n",
    "    url = BASE_URL.format(page_number)\n",
    "    print(f\"scraping page {page_number}...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.encoding = \"utf-8\"\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"failed page {page_number}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "        page_data = []\n",
    "\n",
    "        for book in books:\n",
    "            title = book.h3.a.get(\"title\", \"\").strip()\n",
    "            price_raw = book.find(\"p\", class_=\"price_color\").text.strip()\n",
    "            availability = book.find(\n",
    "                \"p\", class_=\"instock availability\"\n",
    "            ).text.strip()\n",
    "            rating = book.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "\n",
    "            page_data.append({\n",
    "                \"title\": title,\n",
    "                \"price_raw\": price_raw,\n",
    "                \"availability\": availability,\n",
    "                \"rating\": rating,\n",
    "                \"page\": page_number\n",
    "            })\n",
    "\n",
    "        with data_lock:\n",
    "            scraped_data.extend(page_data)\n",
    "\n",
    "        print(f\"page {page_number} done ({len(page_data)} books)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error on page {page_number}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea9a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- PART 2: MULTITHREADING ----------------\n",
    "def run_multithreaded_scraping():\n",
    "    threads = []\n",
    "\n",
    "    for page in range(1, PAGES_TO_SCRAPE + 1):\n",
    "        t = threading.Thread(target=scrape_page, args=(page,))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    print(\"scraping finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f41a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- PART 3: SAVE JSON ----------------\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"saved {len(data)} records to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4bf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- PART 4: DATA ANALYSIS ----------------\n",
    "def analyze_data(filename):\n",
    "    print(\"starting analysis...\")\n",
    "\n",
    "    df = pd.read_json(filename)\n",
    "\n",
    "    # clean price safely\n",
    "    df[\"price_numeric\"] = (\n",
    "        df[\"price_raw\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[^0-9.]\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    df[\"price_numeric\"] = pd.to_numeric(\n",
    "        df[\"price_numeric\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    rating_map = {\n",
    "        \"One\": 1,\n",
    "        \"Two\": 2,\n",
    "        \"Three\": 3,\n",
    "        \"Four\": 4,\n",
    "        \"Five\": 5\n",
    "    }\n",
    "\n",
    "    df[\"rating_score\"] = df[\"rating\"].map(rating_map)\n",
    "\n",
    "    df = df.dropna(subset=[\"price_numeric\", \"rating_score\"])\n",
    "\n",
    "    print(\"total books:\", len(df))\n",
    "    print(\"average price:\", round(df[\"price_numeric\"].mean(), 2))\n",
    "    print(\"5-star books:\", len(df[df[\"rating_score\"] == 5]))\n",
    "\n",
    "    df.to_csv(\"processed_books_data.csv\", index=False)\n",
    "    print(\"processed data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbbf8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1...\n",
      "scraping page 2...\n",
      "scraping page 3...\n",
      "scraping page 4...\n",
      "scraping page 5...\n",
      "page 4 done (20 books)\n",
      "page 5 done (20 books)\n",
      "page 3 done (20 books)\n",
      "page 1 done (20 books)\n",
      "page 2 done (20 books)\n",
      "scraping finished\n",
      "saved 100 records to scraped_books.json\n",
      "starting analysis...\n",
      "total books: 100\n",
      "average price: 34.56\n",
      "5-star books: 19\n",
      "processed data saved\n",
      "execution time: 0.86 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- MAIN ----------------\n",
    "start_time = time.time()\n",
    "\n",
    "run_multithreaded_scraping()\n",
    "save_to_json(scraped_data, OUTPUT_JSON_FILE)\n",
    "analyze_data(OUTPUT_JSON_FILE)\n",
    "\n",
    "print(\"execution time:\", round(time.time() - start_time, 2), \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
